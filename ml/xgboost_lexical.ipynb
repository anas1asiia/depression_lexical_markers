{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb9c500f",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "XGBoost_lexical.ipynb\n",
    "\n",
    "This script helps examine predictive power of lexical features (and TF-IDF) for \n",
    "predicting depression symptom severity (PHQ-8 scores). Developed as part of \n",
    "a study on multilingual lexical markers and depression severity.\n",
    "\n",
    "- Loads and preprocesses multilingual linguistic data (e.g., English, Dutch, Spanish)\n",
    "- Applies feature scaling and text vectorization (TF-IDF)\n",
    "- Constructs XGBoost regression pipelines with and without feature selection\n",
    "- Implements nested GroupKFold cross-validation\n",
    "- Supports multiple feature combinations (lexical features only, features + TF-IDF, etc.)\n",
    "- Evaluates performance using RMSE and R²\n",
    "- Supports grid search for hyperparameter tuning\n",
    "\n",
    "**Usage**:\n",
    "- Place your input CSV file in the desired directory.\n",
    "- Make sure the file includes a column named 'Text' containing the sentences.\n",
    "\n",
    "**Author**: Anastasiia Tokareva\n",
    "\n",
    "\n",
    "\n",
    "### Models tested:\n",
    "1. Baseline: full features, no TF-IDF\n",
    "2. Full features + TF-IDF\n",
    "3. Automatic Feature Selection: SelectKBest() + TSVD + TF-IDF\n",
    "4. Stats-based features + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "38a6f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load libraries\n",
    "\n",
    "# general\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# pre-processing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# ML pipeline\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import GroupKFold, cross_validate, GridSearchCV \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD   # TSVD\n",
    "\n",
    "# regressor\n",
    "# !pip install xgboost\n",
    "from xgboost import XGBRegressor  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb95a35-5484-45e9-ae5e-2db76b2ca177",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Clean your data\n",
    "data = pd.read_csv(\"C:/Users/your/file/name/here.csv\") \n",
    "# remove missing values ('NA')\n",
    "data_cleaned = data.dropna(axis=0) \n",
    "\n",
    "# Below are the changes that were made to rename LIWC-generated column names, which differed \n",
    "# between the languages. Absolutist word frequency could only be extracted for English, so this\n",
    "# lexical feature was not included in the main ML models to enable comparison with Spanish\n",
    "# and Dutch data. Please uncomment the lines that you would like to run for your project.\n",
    "\n",
    "# a) English - remove the column with absolutist words \n",
    "# data_cleaned = data_cleaned.drop('allnone', axis=1)\n",
    "\n",
    "# b) Dutch - rename some columns to match English LIWC column names\n",
    "# data_cleaned = data_cleaned.rename(columns = {'posemo':'emo_pos', 'negemo':'emo_neg'})\n",
    "\n",
    "# c) Spanish - rename some columns to match English LIWC column names\n",
    "# data_cleaned = data_cleaned.rename(columns = {'EmoPos':'emo_pos', 'EmoNeg':'emo_neg', 'Yo':'i', 'Nosotro':'we', 'Pasado': 'focuspast'})\n",
    "\n",
    "\n",
    "## 2. Binarise COVID data\n",
    "data_cleaned['Recording_Date'] = pd.to_datetime(data_cleaned['Recording_Date'])\n",
    "\n",
    "# define COVID lockdown start and end dates (dates based on Leightley et al. (2021), https://pubmed.ncbi.nlm.nih.gov/34488697/)\n",
    "covid_start = pd.to_datetime('2020-03-23')\n",
    "covid_end = pd.to_datetime('2021-05-11')\n",
    "\n",
    "data_cleaned['COVID'] = ((data_cleaned['Recording_Date'] >= covid_start) & (data_cleaned['Recording_Date'] <= covid_end)).astype(int)\n",
    "data_cleaned.head(n=5)\n",
    "\n",
    "# COVID now added as the last column (0/1)\n",
    "data_cleaned.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab7ed22",
   "metadata": {},
   "source": [
    "#### 1. Baseline: lexical features only \n",
    "E.g., frequency of past-tense verbs, WPS, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ced5ffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column transformer (StandardScaler for numerical, and FunctionTransformer for raw features)\n",
    "preprocessor_1 = ColumnTransformer([\n",
    "    ('num_scaler', StandardScaler(), ['Age', 'Education_Years', \n",
    "                                      'emo_neg', 'emo_pos',  'focuspast',\n",
    "                                      'i', 'we',\n",
    "                                      'WPS', 'WC', \n",
    "                                      'TTR','Brunet', \n",
    "                                      'allnone',]),                                        # scale numerical features\n",
    "    ('num_raw', FunctionTransformer(lambda x: x, validate=False), ['Gender', 'COVID'])     # dummy variables unscaled\n",
    "])\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline_1 = Pipeline([\n",
    "    ('preprocessing', preprocessor_1),\n",
    "    ('regressor',  XGBRegressor(random_state = 42))  \n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19696e1c",
   "metadata": {},
   "source": [
    "#### Grid search parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1fddbc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'regressor__max_depth': [3, 5, 7, 9],\n",
    "    'regressor__learning_rate': [0.1, 0.01, 0.001],\n",
    "    'regressor__subsample': [0.5, 0.7, 0.9],        # lowering the value prevents overfitting\n",
    "    'regressor__alpha': [0, 0.01, 0.1, 0.5, 1],     # L1 regularisation (default = 0)\n",
    "    'regressor__lambda': [0, 0.01, 0.1, 0.5, 1]     # L2 regularisation (default = 1)                      \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90880ec",
   "metadata": {},
   "source": [
    "#### 2. Baseline + TF-IDF\n",
    "Lexical features + TF-IDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ed18fa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column transformer (TF-IDF for text, StandardScaler for numerical)\n",
    "preprocessor_2 = ColumnTransformer([\n",
    "    ('tfidf', TfidfVectorizer(max_features=500), 'Text'),                                   # TF-IDF (no scaling)\n",
    "    ('num_scaler', StandardScaler(), ['Age', 'Education_Years', \n",
    "                                      'emo_neg', 'emo_pos',  'focuspast',\n",
    "                                      'i', 'we',\n",
    "                                      'WPS', 'WC', \n",
    "                                      'TTR','Brunet', \n",
    "                                      'allnone',]),                                         # scale numerical features\n",
    "    ('num_raw', FunctionTransformer(lambda x: x, validate=False), ['Gender', 'COVID'])      # keeps dummy variables unscaled\n",
    "])\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline_2 = Pipeline([\n",
    "    ('preprocessing', preprocessor_2),\n",
    "    ('regressor', XGBRegressor(random_state = 42))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3572ff",
   "metadata": {},
   "source": [
    "#### 3. Automated + TF-IDF \n",
    "Selection of features based on their F-score using SelectKBest + dimensionality reduction using TSVD to preserve top 100 componenets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f80e8caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column transformer (StandardScaler for numerical, and FunctionTransformer for raw features)\n",
    "preprocessor_3 = ColumnTransformer([\n",
    "    ('tfidf', TfidfVectorizer(max_features=500), 'Text'),                                      # TF-IDF (no scaling)\n",
    "    ('num_scaler', StandardScaler(), ['Age', 'Education_Years', \n",
    "                                      'emo_neg', 'emo_pos',  'focuspast',\n",
    "                                      'i', 'we',\n",
    "                                      'WPS', 'WC', \n",
    "                                      'TTR','Brunet', \n",
    "                                      'allnone',]),                                            # scale numerical features\n",
    "    ('num_raw', FunctionTransformer(lambda x: x, validate=False), ['Gender', 'COVID'])         # keeps dummy variables unscaled\n",
    "])\n",
    "\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline_3 = Pipeline([\n",
    "    ('preprocessing', preprocessor_3),\n",
    "    ('feature_selection', SelectKBest(f_regression, k=100)),  \n",
    "    ('pca', TruncatedSVD(n_components=100)),  \n",
    "    ('regressor', XGBRegressor(random_state = 42)) \n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ca291c",
   "metadata": {},
   "source": [
    "#### 4. Stats-based (no i, no past) + TF-IDF\n",
    "Removal of lexical features that were not strongly associated with PHQ-8 scores in Linear Mixed Modelling (LME), see lme in this repo for more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6386bdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column transformer (StandardScaler for numerical, and FunctionTransformer for raw features)\n",
    "\n",
    "## In our project, different associations between lexical features and depression severity (PHQ-8 scores) were observed \n",
    "# for different languages. Please uncomment the following section to run the required model.\n",
    "\n",
    "# a) English --> only frequency of past-tense verbs and first-person singular pronouns NOT associated with\n",
    "# PHQ-8 scores --> remove this features\n",
    "\n",
    "preprocessor_4 = ColumnTransformer([\n",
    "    ('tfidf', TfidfVectorizer(max_features=500), 'Text'),                                # TF-IDF (no scaling, as vectors are already scaled)\n",
    "    ('num_scaler', StandardScaler(), ['Age', 'Education_Years', \n",
    "                                      'emo_neg', 'emo_pos',\n",
    "                                      'we',\n",
    "                                      'WPS', 'WC', \n",
    "                                      'TTR','Brunet', \n",
    "                                      'allnone',]),                                      # scale numerical features\n",
    "    ('num_raw', FunctionTransformer(lambda x: x, validate=False), ['Gender', 'COVID'])   # keeps dummy variables unscaled\n",
    "])\n",
    "\n",
    "# b) Dutch --> only WPS and emo_pos were strongly associated with PHQ-8 scores --> remove the rest\n",
    "\n",
    "# preprocessor_4 = ColumnTransformer([\n",
    "#     ('tfidf', TfidfVectorizer(max_features=500), 'Text'),                               # TF-IDF (no scaling, as vectors are already scaled)\n",
    "#     ('num_scaler', StandardScaler(), ['Age', 'Education_Years',\n",
    "#                                     'WPS', 'emo_pos']),                                 # scale numerical features\n",
    "#     ('num_raw', FunctionTransformer(lambda x: x, validate=False), ['Gender', 'COVID'])  # keeps dummy variables unscaled\n",
    "# ])\n",
    "\n",
    "# c) Spanish - no lexical features were strongly associated with PHQ-8 scores --> remove all\n",
    "\n",
    "# preprocessor_4 = ColumnTransformer([\n",
    "#     ('tfidf', TfidfVectorizer(max_features=500), 'Text'),                                # TF-IDF (no scaling, as vectors are already scaled)\n",
    "#     ('num_scaler', StandardScaler(), ['Age', 'Education_Years']),                        # scale numerical features\n",
    "#     ('num_raw', FunctionTransformer(lambda x: x, validate=False), ['Gender', 'COVID'])   # keeps dummy variables unscaled\n",
    "# ])\n",
    "\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline_4 = Pipeline([\n",
    "    ('preprocessing', preprocessor_4),\n",
    "    ('regressor', XGBRegressor(random_state = 42)) \n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f76bf7",
   "metadata": {},
   "source": [
    "### Custom RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "24f36ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom RMSE scorer\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Define the scorers dictionary\n",
    "scorers = {\n",
    "    'rmse': make_scorer(rmse),\n",
    "    'r2': make_scorer(r2_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248a5d1f",
   "metadata": {},
   "source": [
    "### Set up nested CFV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2cdc8993",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Different X for different model versions (see the 4 versions above)\n",
    "\n",
    "# 1) Version 1 (all lexical features)\n",
    "X_1 = data_cleaned[['Age', 'Education_Years', 'Gender', 'COVID',\n",
    "                                      'emo_neg', 'emo_pos',  'focuspast',\n",
    "                                      'i', 'we',\n",
    "                                      'WPS', 'WC', \n",
    "                                      'TTR','Brunet', \n",
    "                                      'allnone',]]    \n",
    "\n",
    "\n",
    "# 2-3) Appraoches 2-3 (all lexical features + TF-IDF)\n",
    "X_23 = data_cleaned[['Age', 'Education_Years', 'Gender', 'COVID',\n",
    "                                      'emo_neg', 'emo_pos',  'focuspast',\n",
    "                                      'i', 'we',\n",
    "                                      'WPS', 'WC', \n",
    "                                      'TTR','Brunet', \n",
    "                                      'allnone', 'Text']]\n",
    "\n",
    "# 4) Significant features only - differs between the languages based on LME results\n",
    "# Please uncomment the section to run it for your project\n",
    "\n",
    "# a) English\n",
    "X_4 = data_cleaned[['Age', 'Education_Years', 'Gender', 'COVID',\n",
    "                                      'emo_neg', 'emo_pos',\n",
    "                                      'we',\n",
    "                                      'WPS', 'WC', \n",
    "                                      'TTR','Brunet', \n",
    "                                      'allnone', 'Text']]  \n",
    "\n",
    "# b) Dutch\n",
    "# X_4 = data_cleaned[['Age', 'Education_Years', 'Gender', 'COVID',\n",
    "#             'Text', 'WPS', 'emo_pos']]\n",
    "\n",
    "# c) Spanish\n",
    "# X_4 = data_cleaned[['Age', 'Education_Years', 'Gender', 'COVID','Text', ]]\n",
    "\n",
    "\n",
    "# y = data_cleaned[['PHQ8']]\n",
    "y = data_cleaned['PHQ8'].values.ravel()  # This will convert y to a 1D array\n",
    "groups = data_cleaned['participant_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c56c1cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the outer cross-validation strategy (GroupKFold)\n",
    "inner_cv = GroupKFold(n_splits=5)\n",
    "outer_cv = GroupKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275102bb",
   "metadata": {},
   "source": [
    "#### 1. Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16783994-a572-451c-ab44-6f0248e7b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner Loop\n",
    "Inner_Grid = GridSearchCV(pipeline_1,\n",
    "                          param_grid,\n",
    "                          verbose = 1,\n",
    "                          cv=inner_cv,\n",
    "                          refit='rmse',\n",
    "                          return_train_score=True  \n",
    "                         )\n",
    "\n",
    "# Outer Loop\n",
    "nested_results = cross_validate(Inner_Grid, X_1, y, \n",
    "                                cv=outer_cv,\n",
    "                                groups=groups,\n",
    "                                params={'groups': groups},  # pass group information to inner split \n",
    "                                scoring=scorers,\n",
    "                                return_train_score=True)    # optionally return train scores\n",
    "\n",
    "print(f\"Average Inner RMSE: {np.mean(nested_results['train_rmse']):.2f}\")\n",
    "print(f\"Average Inner R²: {np.mean(nested_results['train_r2']):.2f}\")\n",
    "print(f\"Average Outer RMSE: {np.mean(nested_results['test_rmse']):.2f}\")\n",
    "print(f\"Average Outer R²: {np.mean(nested_results['test_r2']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3885ec23",
   "metadata": {},
   "source": [
    "#### 2. Baseline + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81ed193-3099-492a-8281-f3ba926746a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner Loop\n",
    "Inner_Grid = GridSearchCV(pipeline_2,\n",
    "                          param_grid,\n",
    "                          verbose = 1,\n",
    "                          cv=inner_cv,\n",
    "                          refit='rmse',\n",
    "                          return_train_score=True  \n",
    "                         )\n",
    "\n",
    "# Outer Loop\n",
    "nested_results = cross_validate(Inner_Grid, X_23, y, \n",
    "                                cv=outer_cv,\n",
    "                                groups=groups,\n",
    "                                params={'groups': groups},  # pass group information to inner split \n",
    "                                scoring=scorers,\n",
    "                                return_train_score=True)    # optionally return train scores\n",
    "\n",
    "print(f\"Average Inner RMSE: {np.mean(nested_results['train_rmse']):.2f}\")\n",
    "print(f\"Average Inner R²: {np.mean(nested_results['train_r2']):.2f}\")\n",
    "print(f\"Average Outer RMSE: {np.mean(nested_results['test_rmse']):.2f}\")\n",
    "print(f\"Average Outer R²: {np.mean(nested_results['test_r2']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09894243",
   "metadata": {},
   "source": [
    "#### 3. Automated + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fd03a1-546d-4d8d-84c8-af31dbdb76d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner Loop\n",
    "Inner_Grid = GridSearchCV(pipeline_3,\n",
    "                          param_grid,\n",
    "                          verbose = 1,\n",
    "                          cv=inner_cv,\n",
    "                          refit='rmse',\n",
    "                          return_train_score=True  \n",
    "                         )\n",
    "\n",
    "nested_results = cross_validate(Inner_Grid, X_23, y, \n",
    "                                cv=outer_cv,\n",
    "                                groups=groups,\n",
    "                                params={'groups': groups},  # pass group information to inner split \n",
    "                                scoring=scorers,\n",
    "                                return_train_score=True)    # optionally return train scores\n",
    "\n",
    "print(f\"Average Inner RMSE: {np.mean(nested_results['train_rmse']):.2f}\")\n",
    "print(f\"Average Inner R²: {np.mean(nested_results['train_r2']):.2f}\")\n",
    "print(f\"Average Outer RMSE: {np.mean(nested_results['test_rmse']):.2f}\")\n",
    "print(f\"Average Outer R²: {np.mean(nested_results['test_r2']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f711941f",
   "metadata": {},
   "source": [
    "#### 4. Stats-based + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc49821-3dd4-4702-8617-c15ed94e75e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner Loop\n",
    "Inner_Grid = GridSearchCV(pipeline_4,\n",
    "                          param_grid,\n",
    "                          verbose = 1,\n",
    "                          cv=inner_cv,\n",
    "                          refit='rmse',\n",
    "                          return_train_score=True  \n",
    "                         )\n",
    "\n",
    "nested_results = cross_validate(Inner_Grid, X_4, y, \n",
    "                                cv=outer_cv,\n",
    "                                groups=groups,\n",
    "                                params={'groups': groups},  # pass group information to inner split \n",
    "                                scoring=scorers,\n",
    "                                return_train_score=True)    # optionally return train scores\n",
    "\n",
    "print(f\"Average Inner RMSE: {np.mean(nested_results['train_rmse']):.2f}\")\n",
    "print(f\"Average Inner R²: {np.mean(nested_results['train_r2']):.2f}\")\n",
    "print(f\"Average Outer RMSE: {np.mean(nested_results['test_rmse']):.2f}\")\n",
    "print(f\"Average Outer R²: {np.mean(nested_results['test_r2']):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
