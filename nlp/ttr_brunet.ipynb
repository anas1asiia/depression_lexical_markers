{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e44c60-c467-4266-8750-66b971375a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "lexical_features.ipynb\n",
    "\n",
    "This script calculates lexical diversity metrics (TTR and Brunet's Index)\n",
    "from a dataset of spoken language transcripts. Developed as part of a study\n",
    "on multilingual lexical markers and depression severity.\n",
    "\n",
    "Usage:\n",
    "- Place your input CSV file in the /data folder.\n",
    "- Ensure the file contains a column named 'Text' with participant responses.\n",
    "- Run the script after setting the correct working directory and file name.\n",
    "\n",
    "Author: Anastasiia Tokareva\n",
    "\"\"\"\n",
    "\n",
    "# Set your working directory\n",
    "data_dir = \"you/directory/here\"\n",
    "os.chdir(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efdfc6a-bd9e-45c6-99e0-a23716f61bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Load libraries, set wd, and load data\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Install and import the NLTK library for natural language processing\n",
    "! pip install nltk\n",
    "import nltk\n",
    "\n",
    "# Donwload Punkt tokenizer (tokenises text into sentebces and sentences into words)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('your_speech_transcript_dataset.csv')  # enter the name of your file\n",
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4a53a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Calculate TTR for each row and add it as a new column\n",
    "# TTR = n_unique_words/n_total_words * 100%\n",
    "\n",
    "def calculate_ttr(text):\n",
    "    if isinstance(text, str):                       # condition: TTR calculated if the entry is string\n",
    "        tokens = nltk.word_tokenize(text.lower())   # tokenize and convert to lowercase\n",
    "        unique_words = set(tokens)                  # convert a list of tokens into a set\n",
    "        n_unique_words = len(unique_words)          # ccount unqiue words\n",
    "        n_total_words = len(tokens)                 # count total words in the response\n",
    "        ttr = (n_unique_words / n_total_words) * 100 if n_total_words > 0 else 0   # no /0 if the entry is empty\n",
    "        return ttr\n",
    "    else:\n",
    "        return 0  # condition: otherwise, return 0\n",
    "\n",
    "# Apply the TTR calculation to the 'Text' column - change the column name if needed\n",
    "data['TTR'] = data['Text'].apply(calculate_ttr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bb2accd-e1ef-42e9-aa44-1710400ac9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated csv file saved as RADAR_LIWC_Brunet.csv\n"
     ]
    }
   ],
   "source": [
    "## 3. Calculate Brunet's index for each row and add it as a new columnBrunet index of LD\n",
    "# W = N**V**-alpha, where N = text total text length, V = number of unique words\n",
    "\n",
    "alpha = -0.165   # standard value used\n",
    "\n",
    "def calculate_brunet(text, alpha=alpha):\n",
    "    if isinstance(text, str): \n",
    "        tokens = nltk.word_tokenize(text.lower())  \n",
    "        unique_words = set(tokens)\n",
    "        n_unique_words = len(set(tokens))\n",
    "        n_total_words = len(tokens)\n",
    "        \n",
    "        if n_total_words > 0 and n_unique_words > 0:\n",
    "            brunet_index = n_total_words ** (n_unique_words ** -alpha)\n",
    "            return brunet_index\n",
    "        else:\n",
    "            return 0   # if no words\n",
    "    else:\n",
    "        return 0     # if not a string\n",
    "\n",
    "\n",
    "# Apply the Brunet index calculation to the 'Text' column - change column name if needed\n",
    "data['Brunet'] = data['Text'].apply(calculate_brunet)\n",
    "\n",
    "# Save the updated file as a new csv file in the same wd\n",
    "output_file = 'RADAR_LIWC_Brunet.csv'\n",
    "data.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Updated csv file saved as {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
